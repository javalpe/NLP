{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sesion6_Introducción al Procesamiento de Lenguaje Natural.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf8amewP3xeb",
        "colab_type": "text"
      },
      "source": [
        "# Dataset de Tweets para Análisis de Sentimientos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSYIsxjS4A5M",
        "colab_type": "text"
      },
      "source": [
        "## Capturamos un train y un test de tweets previamente seleccionados por una competencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUKslD8M4eHG",
        "colab_type": "text"
      },
      "source": [
        "Los detalles de la competencia **gratuita** pueden encontrarse en la siguiente web: [https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_VJey44TK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/train_twitter_analysis.csv')\n",
        "test=pd.read_csv('https://raw.githubusercontent.com/javalpe/datasets/master/test_twitter_analysis.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RoCM66n6aOb",
        "colab_type": "text"
      },
      "source": [
        "## Importamos las librerías, denegamos alertas y seteamos la visualización del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgLxZwwc598f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re    # regular expression nos permite editar la visualización de los caracteres según nuestras prefencias\n",
        "import nltk  # natural language tokenization es una librería para manipular las palabras que encontraremos en los tweets\n",
        "import string # exclusivamente para palabras \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdu62J_C6RJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Definimos un máximo de ancho de columna por la naturaleza del largo de los textos\n",
        "pd.set_option(\"display.max_colwidth\", 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpu9IpCj7_kE",
        "colab_type": "text"
      },
      "source": [
        "## Entendemos la data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3wDEKCMEJYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizamos el tamaño de cada dataset con el comando shape\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnHvydj48OdD",
        "colab_type": "text"
      },
      "source": [
        "La competencia nos pide predecir si un tweet es **racista o sexista** y para ello se le ha asignado un valor 1 o 0 en la columna **label** (columna objetivo en train).\n",
        "Veamos cada sección de tweets según esta característica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkDrRcB57-JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[train['label'] == 0].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj5eXXYm83SH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[train['label'] == 1].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxxrjfhL9N1P",
        "colab_type": "text"
      },
      "source": [
        "Podemos distinguir **cuántos** de los tweets en train son racistas o sexistas y cuántos no con un simple *value_counts* de la columna label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvBqSPsy9Iqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"label\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwclnQ1K91mn",
        "colab_type": "text"
      },
      "source": [
        "Solo el 7% de los datos del dataset train contienen tweets clasificados como racista o sexista. Esto se conoce como **data desbalanceada** y existen técnicas para solucionar este problema. Pueden encontrar mayor información en [https://www.kdnuggets.com/2019/05/fix-unbalanced-dataset.html](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLUIj2Q_k6l",
        "colab_type": "text"
      },
      "source": [
        "**También podemos visualizar gráficamente el largo de los tweets** Para ello usamos *str.len* sobre cada uno de nuestros dataset: train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_21L6w_j97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_train = train['tweet'].str.len() \n",
        "length_test = test['tweet'].str.len() \n",
        "plt.hist(length_train, bins=20, label=\"train_tweets\") #el comando hist de la librería matplotlib (plt) permite dibujar histogramas\n",
        "plt.hist(length_test, bins=20, label=\"test_tweets\") #el parámetro label es para identificar estos datos para una leyenda del gráfico\n",
        "plt.legend() #el comando legend permite añadir una leyenda a nuestra gráfica\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E83TVihAthX",
        "colab_type": "text"
      },
      "source": [
        "Podemos visualizar que el largo de los tweets de train es **significativamente mayor** que los tweets de test. El primero llega a un máximo de 6,000 caracteres por tweet, mientras que el segundo a un máximo de 3,000 caracteres por tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7XWodKNBXoF",
        "colab_type": "text"
      },
      "source": [
        "## Tratamiento de la data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFUJZQGqBkqT",
        "colab_type": "text"
      },
      "source": [
        "En esta sección procederemos a eliminar de los datos los caracteres que nos impiden clasificar mejor los tweets (por ejemplo signos de puntuación)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFN0PppRAmbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inicialmente juntamos train y test para realizar este tratamiento para ambos dataset. Luego volveremos a dividirlos\n",
        "combi = train.append(test, ignore_index=True) \n",
        "combi.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3JWRtA1B_UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Definimos una función para eliminar de los tweets alguna palabra, simbolo o caracter especial (que irá en el parámetro pattern) que creamos conveniente\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abl-jc8ODmfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Quitamos todas aquellas menciones dentro de los tweets a otros usuarios (que comienzan con @)\n",
        "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")  #llamamos la función remove_pattern y el parámetro pattern será @[\\w]*\n",
        "combi.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0IxTDBHFqPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reemplazamos todos aquellos carácteres especiales como signos de puntuación por un espacio en blanco\n",
        "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \") #el simbolo ^ signfica \"todo menos\" en este caso palabras y números \n",
        "combi.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB255XNiGf6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removemos todas aquellas palabras con un largo de 3 caracteres o menos que podrían incluir conectores, advervios, artículos (por ejemplo a, and, the)\n",
        "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "combi.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAsClMSCI0mM",
        "colab_type": "text"
      },
      "source": [
        "## Tokenización y Steamming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO7z6_TSI9Xf",
        "colab_type": "text"
      },
      "source": [
        "Para poder evaluar cada tweet será necesario dividirlo en palabras, este proceso se denomina *tokenization*.\n",
        "\n",
        "También realizaremos el proceso denominado *steamming* para solo utilizar las raíces de las palabras en el análisis (por ejemplo \"read\" en vez de \"reading\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6gPUqADHjR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dividimos los tweets en palabras y guardamos el resultado en una variable denominada tokenized_tweet\n",
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) #el comando split signfica dividir\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9jgL_TfKA3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ahora procederemos a extraer solo la raíz de las palabras para un mejor análisis y actualizamos nuestra variable tokenized_tweet\n",
        "from nltk.stem.porter import * \n",
        "stemmer = PorterStemmer() \n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0ZXUEwqKiwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finalmente juntamos cada raíz de palabra para almacenarla nuevamente en la columna tidy_tweet\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \n",
        "combi['tidy_tweet'] = tokenized_tweet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVx12ijWL8iL",
        "colab_type": "text"
      },
      "source": [
        "## Visualización de palabras más usadas y hashtag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV7doBonMDx_",
        "colab_type": "text"
      },
      "source": [
        "En esta sección utilizaremos el método **wordcloud** para identificar palabras más usadas\n",
        "\n",
        "Además visualizaremos histogramas para identificar los hashtag (#) más usados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To6GdGrDOFkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importamos la librería WordCloud para visualizar las palabras más usadas (que aparecen en mayor tamaño) en la columna tidy_tweet completa que denominamos all_words\n",
        "from wordcloud import WordCloud\n",
        "all_words = ' '.join([text for text in combi['tidy_tweet']])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words) \n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_x03dzfLABc",
        "colab_type": "text"
      },
      "source": [
        "Repetimos el mismo análisis **solo para aquellos tweets que estén caracterizados como no racistas ni sexistas** (label=0) que denominamos normal_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcf4GrYKNaNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]]) \n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWhfb8l5LJrN",
        "colab_type": "text"
      },
      "source": [
        "Y por último realizamos el mismo análisis para aquellos **tweets que estén caracterizados como racistas ni sexistas** (label=1) que denominamos negative_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aRZs-RgOmYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "negative_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]]) \n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1At4vrlL0KU",
        "colab_type": "text"
      },
      "source": [
        "Para argumentar con **números** el impacto de las palabras dentro del dataset, train y test, vamos a realizar un **gráfico de barras** con ayuda del método displot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg34kkGKO9K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos una función que nos permita extraer los hashtags (#) de los tweets\n",
        "def hashtag_extract(x):\n",
        "    hashtags = []\n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "    return hashtags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPZPRWdEovQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extraemos los hashtags (#) de los tweets que no son racistas ni sexistas (label=0) y guardamos el resultado en la variable HT_regular\n",
        "HT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKsMwcnWozED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extraemos los hashtags (#) de los tweets que sí son racistas ni sexistas (label=1) y guardamos el resultado en la variable HT_negative \n",
        "HT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETr9667Co1d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Para poder realizar un conteo y graficarlo en barras procedemos a unir todas las palabras de cada variable\n",
        "HT_regular = sum(HT_regular,[])\n",
        "HT_negative = sum(HT_negative,[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMYvLjxjqI2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Con ayuda del método FreqDist de realizamos el conteo y graficamos las frecuencias de aparición de cada hashtag en los tweets de la variable HT_regular (label=0)\n",
        "a = nltk.FreqDist(HT_regular)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "d = d.nlargest(columns=\"Count\", n = 10) #Para efectos de visualización solo nos quedaremos con el top10\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\") #Para\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtX9ZK2gs5ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Con ayuda del método FreqDist de realizamos el conteo y graficamos las frecuencias de aparición de cada hashtag en los tweets de la variable HT_negative (label=1)\n",
        "b = nltk.FreqDist(HT_negative)\n",
        "e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n",
        "e = e.nlargest(columns=\"Count\", n = 10)   #Para efectos de visualización solo nos quedaremos con el top10\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--zyZweqJtW4",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesamiento de la data ( I ): Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2lKhxkEJ1Lk",
        "colab_type": "text"
      },
      "source": [
        "El primer método para preparar la data y poder ejecutar modelos de clasificación se denomina **Bag of Words** donde se obtiene un dataset compuesto por D documentos (que representan filas o registros) y N corpus (que representan las variables predictoras)\n",
        "\n",
        "Para nuestro ejemplo el objetivo es obtener como variables predictoras (columnas o *features*) las palabras y las filas o registros serán cada tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGjHab30Jsc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importamos la librería CountVectorizer de sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') #en nuestro caso estamos procesando tweets en inglés\n",
        "bow = bow_vectorizer.fit_transform(combi['tidy_tweet']) #guardamos el resultado en una variable denominada bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zztKR0Z9Lq6c"
      },
      "source": [
        "## Preprocesamiento de la data ( II ): TF - IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb0_wIoULqZR",
        "colab_type": "text"
      },
      "source": [
        "El segundo método para preparar la data y poder ejecutar modelos de clasificación se denomina **TF - IDF** donde se obtiene un dataset conformado por los mismos elementos. Sin embargo vamos a añadir dos elementos de evaluación (como pesos para cada variable):\n",
        "\n",
        "\n",
        "1.   TF = # veces que aparece una palabra en un solo documento / # palabras totales de ese mismo documento\n",
        "2.   IDF = log (N/n) n es el # veces que aparece una palabra en cada documento y N es el # total de documentos\n",
        "\n",
        "\n",
        "Para nuestro ejemplo recordemos que un documento es un tweet y una palabra es cada una de las palabras que hemos tokenizado y steammeado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If_A9dU2N-Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importamos la librería TfidVectorizer de sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english') #en nuestro caso estamos procesando tweets en inglés\n",
        "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet']) #guardamos el resultado en una variable denominada tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKYNs5D0Q0AS"
      },
      "source": [
        "## Preprocesamiento de la data ( III ): Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "62ZB0EbFQ4Q-"
      },
      "source": [
        "El tercer método para preparar la data y poder ejecutar modelos de clasificación se denomina **Word2Vec** donde cada palabra se transforma en un vector para relacionar las palabras entre sí antes de generar el dataset. Esto nos trae dos ventajas:\n",
        "\n",
        "\n",
        "1.   Se reduce significativamente la cantidad de columnas generadas\n",
        "2.   Se puede brindar dos significados distintos para una misma palabra (por ejemplo *apple* puede significar fruta y también signficar empresa tecnológica)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnK_Hmo-OfF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dividimos nuevamente la columna tidy_tweet palabra por palabra para poder realizar el análisis y guardamos el resultado en la variable tokenized_tweet\n",
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWtEOV74UPDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importamos las librerías que necesitamos para ejecutar el análisis Word2vec\n",
        "import gensim\n",
        "from gensim.models.doc2vec import LabeledSentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhIvVLSZT1ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generamos un modelo word2vec y guardamos el modelo en la variable model_w2v\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            tokenized_tweet,\n",
        "            size=200,\n",
        "            window=5,\n",
        "            min_count=2,\n",
        "            sg = 1,\n",
        "            hs = 0,\n",
        "            negative = 10,\n",
        "            workers= 2,\n",
        "            seed = 34)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTZ5CAEBZz2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Entrenamos el modelo con los tweets tokenizados\n",
        "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1C1Gi2vaQpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Probamos el modelo entrenado para encontrar las palabras más relacionadas con la palabra \"dinner\"\n",
        "model_w2v.wv.most_similar(positive=\"dinner\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tTtBYInjWMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Definimos una función para generar un vector para cada uno de los tweet\n",
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0.\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError: continue\n",
        "        if count != 0: vec/= count\n",
        "    return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opRbFRDxmk6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplicamos la función sobre cada tweet y generamos un dataframe que guardaremos como la variable wordvec_df\n",
        "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
        "for i in range(len(tokenized_tweet)):\n",
        "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
        "    wordvec_df = pd.DataFrame(wordvec_arrays)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Ij_n_Jm-ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Comprobamos el tamaño de la variable y visulizamos un total de 200 columnas\n",
        "wordvec_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjSpeJnBnI5S",
        "colab_type": "text"
      },
      "source": [
        "Este último método nos ha brindado un dataset de 200 columnas, mientras que los anteriores métodos (BagOfWords y TF-IDF) resultan en promedio 1000 columnas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Fd9KHHKyh6",
        "colab_type": "text"
      },
      "source": [
        "## Regresión Logística a través de Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7EUCSz-KxCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importamos las librerías necesarias para efectar la regresión logística\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#Actualmente bow tiene guardado todos los tweets (ya procesados según Bag Of Words) por ello lo separamos en train y test según el shape que efectuamos al inicio\n",
        "train_bow = bow[:31962,:]\n",
        "test_bow = bow[31962:,:]\n",
        "\n",
        "#Utilizamos el método train_test_split para dividir nuestra data de entrenamiento (train_bow) en train y valid\n",
        "xtrain_bow, xvalid_bow, ytrain_bow, yvalid_bow = train_test_split(train_bow, train['label'], test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zDnm0_eGfWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparamos el modelo regresión logística solo en base a los datos de entrenamiento (xtrain_bow, ytrain_bow)\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain_bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXOQj7KOGouZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtenemos las predicciones (probabilidades) del modelo de regresión logística utilizando los datos de validación (xvalid_bow)\n",
        "prediction_bow = lreg.predict_proba(xvalid_bow)\n",
        "prediction_int_bow = prediction_bow[:,1] >= 0.195 #Establecemos el punto de corte en 0.195\n",
        "prediction_int_bow = prediction_int_bow.astype(np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcZOgaxEGx6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculamos el indicador f1_score del modelo contrastando las predicciones con los target de validación (yvalid_bow)\n",
        "f1_bow=f1_score(yvalid_bow, prediction_int_bow)\n",
        "f1_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as6Hb_2fLEqY",
        "colab_type": "text"
      },
      "source": [
        "## Regresión Logística a través de TF -IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2hoiEzLL2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Actualmente tfidf tiene guardado todos los tweets (ya procesados según TF-IDF) por ello lo separamos en train y test según el shape que efectuamos al inicio\n",
        "train_tfidf = tfidf[:31962,:]\n",
        "test_tfidf = tfidf[31962:,:]\n",
        "\n",
        "#Utilizamos el método train_test_split para dividir nuestra data de entrenamiento (train_tfidf) en train y valid\n",
        "xtrain_tfidf, xvalid_tfidf, ytrain_tfidf, yvalid_tfidf = train_test_split(train_tfidf, train['label'], test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oY-z7plLy2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparamos el modelo regresión logística solo en base a los datos de entrenamiento (xtrain_tfidf, ytrain_tfidf)\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_tfidf, ytrain_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAl_3pHqLzG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtenemos las predicciones (probabilidades) del modelo de regresión logística utilizando los datos de validación (xvalid_tfidf)\n",
        "prediction_tfidf = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int_tfidf = prediction_tfidf[:,1] >= 0.295 #Establecemos el punto de corte en 0.295\n",
        "prediction_int_tfidf = prediction_int_tfidf.astype(np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIj7vURhLzYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculamos el indicador f1_score del modelo contrastando las predicciones con los target de validación (yvalid_tfidf)\n",
        "f1_tfidf=f1_score(yvalid_tfidf, prediction_int_tfidf)\n",
        "f1_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LUooulQUQkQv"
      },
      "source": [
        "## Regresión Logística a través de Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ0PhQiyQj2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Actualmente wordvec_df tiene guardado todos los tweets (ya procesados según word2vec) por ello lo separamos en train y test según el shape que efectuamos al inicio\n",
        "train_w2v = wordvec_df.iloc[:31962,:]\n",
        "test_w2v = wordvec_df.iloc[31962:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38eaomHDRIj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Utilizamos el método train_test_split para dividir nuestra data de entrenamiento (train_w2v) en train y valid\n",
        "xtrain_w2v, xvalid_w2v, ytrain_w2v, yvalid_w2v = train_test_split(train_w2v, train['label'], test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iyctXiPwSQXL",
        "colab": {}
      },
      "source": [
        "#Preparamos el modelo regresión logística solo en base a los datos de entrenamiento (xtrain_w2v, ytrain_w2v)\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_w2v, ytrain_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uChw0KL8R_cM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtenemos las predicciones (probabilidades) del modelo de regresión logística utilizando los datos de validación (xvalid_w2v)\n",
        "prediction_w2v = lreg.predict_proba(xvalid_w2v)\n",
        "prediction_int_w2v = prediction_w2v[:,1] >= 0.1989 #Establecemos el punto de corte en 0.295\n",
        "prediction_int_w2v = prediction_int_w2v.astype(np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxghEL8bTAkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculamos el indicador f1_score del modelo contrastando las predicciones con los target de validación (yvalid_w2v)\n",
        "f1_w2v=f1_score(yvalid_w2v, prediction_int_w2v)\n",
        "f1_w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUSy7Z5GXYqK",
        "colab_type": "text"
      },
      "source": [
        "## RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YakAW14fdQyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5JJ-d2ba75k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplicamos el modelo de RandomForest para los datos procesados según Bag of Words y calculamos el indicador f1_score\n",
        "rf_bow = RandomForestClassifier(n_estimators=400).fit(xtrain_bow, ytrain_bow)\n",
        "prediction_rf_bow = rf_bow.predict(xvalid_bow)\n",
        "f1_rf_bow=f1_score(yvalid_bow, prediction_rf_bow)\n",
        "f1_rf_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GggwVCHoXk8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplicamos el modelo de RandomForest para los datos procesados según TF-IDF y calculamos el indicador f1_score\n",
        "rf_tfidf = RandomForestClassifier(n_estimators=400).fit(xtrain_tfidf, ytrain_tfidf)\n",
        "prediction_rf_tfidf = rf_tfidf.predict(xvalid_tfidf)\n",
        "f1_rf_tfidf=f1_score(yvalid_tfidf, prediction_rf_tfidf)\n",
        "f1_rf_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TumbQszYYaM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplicamos el modelo de RandomForest para los datos procesados según Word2Vec y calculamos el indicador f1_score\n",
        "rf_w2v = RandomForestClassifier(n_estimators=400).fit(xtrain_w2v, ytrain_w2v)\n",
        "prediction_rf_w2v = rf_w2v.predict(xvalid_w2v)\n",
        "f1_rf_w2v=f1_score(yvalid_w2v, prediction_rf_w2v)\n",
        "f1_rf_w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSQC7AXFeJd6"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9Q5LOFpeMOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qP8C8UpeQv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Aplicamos el modelo de XGBClassifier para los datos procesados según Bag of Words y calculamos el indicador f1_score\n",
        "xgbc = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain_bow) \n",
        "prediction_xgbc_bow = xgbc.predict(xvalid_bow)\n",
        "f1_xgbc_bow=f1_score(yvalid_bow, prediction_xgbc_bow)\n",
        "f1_xgbc_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMEF40ftfXVK",
        "colab": {}
      },
      "source": [
        "#Aplicamos el modelo de XGBClassifier para los datos procesados según TF-IDF y calculamos el indicador f1_score\n",
        "xgbc = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain_tfidf) \n",
        "prediction_xgbc_tfidf = xgbc.predict(xvalid_tfidf)\n",
        "f1_xgbc_tfidf=f1_score(yvalid_tfidf, prediction_xgbc_tfidf)\n",
        "f1_xgbc_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gSZRyY2JfmAu",
        "colab": {}
      },
      "source": [
        "#Aplicamos el modelo de XGBClassifier para los datos procesados según Word2Vec y calculamos el indicador f1_score\n",
        "xgbc = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_w2v, ytrain_w2v) \n",
        "prediction_xgbc_w2v = xgbc.predict(xvalid_w2v)\n",
        "f1_xgbc_w2v=f1_score(yvalid_w2v, prediction_xgbc_w2v)\n",
        "f1_xgbc_w2v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1Ul4MJc_g9Fy"
      },
      "source": [
        "## Mejorando XGBoost + Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFttueaJg5zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAzwZA9bhFRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain_w2v) \n",
        "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid_w2v) \n",
        "dtest = xgb.DMatrix(test_w2v)\n",
        "# Parameters that we are going to tune \n",
        "params = {\n",
        "    'objective':'binary:logistic',\n",
        "    'max_depth':6,\n",
        "    'min_child_weight': 1,\n",
        "    'eta':.3,\n",
        "    'subsample': 1,\n",
        "    'colsample_bytree': 1\n",
        " }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPGiv3-ahGoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_eval(preds, dtrain):\n",
        "    labels = dtrain.get_label().astype(np.int)\n",
        "    preds = (preds >= 0.3).astype(np.int)\n",
        "    return [('f1_score', f1_score(labels, preds))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjUzUX42hN5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(6,10)\n",
        "     for min_child_weight in range(5,8)\n",
        " ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGVm42RMhR7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0\n",
        "best_params = None \n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "    \n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    cv_results = xgb.cv(params,\n",
        "        dtrain,        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    \n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "        \n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()    \n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "      max_f1 = mean_f1\n",
        "      best_params = (max_depth,min_child_weight) \n",
        "      print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwkSat5KhtLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params['max_depth'] = 8 \n",
        "params['min_child_weight'] = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFosidU2idnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch_params = [\n",
        "    (subsample, colsample)\n",
        "    for subsample in [i/10. for i in range(5,10)]\n",
        "    for colsample in [i/10. for i in range(5,10)] ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHhgUTQNhyTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0\n",
        "best_params = None \n",
        "for subsample, colsample in gridsearch_params:\n",
        "    print(\"CV with subsample={}, colsample={}\".format(subsample,colsample))\n",
        "    \n",
        "    params['colsample'] = colsample\n",
        "    params['subsample'] = subsample\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=200,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "    \n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "      max_f1 = mean_f1\n",
        "      best_params = (subsample, colsample) \n",
        "      print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhbZOJCyhz2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params['subsample'] = .9 \n",
        "params['colsample_bytree'] = .5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbh9HIQSh2sL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_f1 = 0. \n",
        "best_params = None \n",
        "for eta in [.3, .2, .1, .05, .01, .005]:\n",
        "    print(\"CV with eta={}\".format(eta))\n",
        "    \n",
        "    params['eta'] = eta\n",
        "\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        feval= custom_eval,\n",
        "        num_boost_round=1000,\n",
        "        maximize=True,\n",
        "        seed=16,\n",
        "        nfold=5,\n",
        "        early_stopping_rounds=20\n",
        "    )\n",
        "\n",
        "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
        "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
        "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
        "    if mean_f1 > max_f1:\n",
        "      max_f1 = mean_f1\n",
        "      best_params = eta\n",
        "      print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2iotJN1iAox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params['eta'] = .1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIDBKEuGiBbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params\n",
        "{'colsample': 0.9,\n",
        " 'colsample_bytree': 0.5, 'eta': 0.1,\n",
        " 'max_depth': 8, 'min_child_weight': 6,\n",
        " 'objective': 'binary:logistic',\n",
        " 'subsample': 0.9}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2_L7-vPiKqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    feval= custom_eval,\n",
        "    num_boost_round= 1000,\n",
        "    maximize=True,\n",
        "    evals=[(dvalid, \"Validation\")],\n",
        "    early_stopping_rounds=10\n",
        " )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH6E0_LKiLZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = xgb_model.predict(dtest)\n",
        "test['label'] = (test_pred >= 0.3).astype(np.int)\n",
        "submission = test[['id','label']] \n",
        "submission.to_csv('sub_xgb_w2v_finetuned.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}